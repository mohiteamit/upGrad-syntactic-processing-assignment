{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom NER for Identifying Diseases and Treatments\n",
    "\n",
    "This notebook implements a custom Named Entity Recognition (NER) system to identify diseases and treatments from a medical dataset. The dataset is provided in tokenized format, where each word is associated with a label:\n",
    "- `O` indicates \"Other\"\n",
    "- `D` indicates \"Disease\"\n",
    "- `T` indicates \"Treatment\"\n",
    "\n",
    "## Steps in this Notebook\n",
    "1. **Data Preprocessing:** Reconstruct sentences and labels from the tokenized dataset.\n",
    "2. **Concept Identification:** Identify key concepts in the dataset using PoS tagging.\n",
    "3. **Defining Features for CRF:** Create features for training the CRF model.\n",
    "4. **Getting Features for Words and Sentences:** Apply feature definitions to all sentences.\n",
    "5. **Defining Input and Target Variables:** Prepare input features and labels for training and testing.\n",
    "6. **Building the Model:** Train the CRF model on the training dataset.\n",
    "7. **Evaluating the Model:** Evaluate the model on the test dataset using F1 score and classification metrics.\n",
    "8. **Identifying Diseases and Predicted Treatments:** Extract relationships between diseases and treatments using the trained model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data Preprocessing\n",
    "The dataset is provided in tokenized format, where each word is stored on a separate line, and sentences are separated by blank lines. In this step, I will:\n",
    "1. Reconstruct sentences and labels from the training and testing datasets.\n",
    "2. Count the number of sentences and labels in the processed datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in train dataset: 2599\n",
      "Number of sentences in test dataset: 1056\n",
      "Number of label lines in train dataset: 2599\n",
      "Number of label lines in test dataset: 1056\n"
     ]
    }
   ],
   "source": [
    "# Paths to the dataset files\n",
    "train_sent_path = 'data/train_sent'\n",
    "train_label_path = 'data/train_label'\n",
    "test_sent_path = 'data/test_sent'\n",
    "test_label_path = 'data/test_label'\n",
    "\n",
    "def process_data(file_path):\n",
    "    \"\"\"\n",
    "    Read a dataset file and reconstruct sentences or labels.\n",
    "\n",
    "    Parameters:\n",
    "    file_path (str): Path to the file containing data in tokenized format.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of sentences or labels reconstructed from the file.\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    current_sentence = []\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line == \"\":  # A blank line indicates the end of a sentence\n",
    "                if current_sentence:\n",
    "                    sentences.append(current_sentence)\n",
    "                    current_sentence = []\n",
    "            else:\n",
    "                current_sentence.append(line)\n",
    "        if current_sentence:  # Add the last sentence if the file does not end with a blank line\n",
    "            sentences.append(current_sentence)\n",
    "\n",
    "    return sentences\n",
    "\n",
    "# Process train and test datasets\n",
    "train_sentences = process_data(train_sent_path)\n",
    "train_labels = process_data(train_label_path)\n",
    "test_sentences = process_data(test_sent_path)\n",
    "test_labels = process_data(test_label_path)\n",
    "\n",
    "# Verify by printing counts\n",
    "print(f\"Number of sentences in train dataset: {len(train_sentences)}\")\n",
    "print(f\"Number of sentences in test dataset: {len(test_sentences)}\")\n",
    "print(f\"Number of label lines in train dataset: {len(train_labels)}\")\n",
    "print(f\"Number of label lines in test dataset: {len(test_labels)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Concept Identification\n",
    "In this step, I will identify key concepts (e.g., diseases and treatments) from the dataset by:\n",
    "1. Performing Part-of-Speech (PoS) tagging on the text data.\n",
    "2. Extracting tokens with PoS tags corresponding to nouns (`NOUN` and `PROPN`).\n",
    "3. Counting the frequency of these tokens across the entire dataset (both training and testing data).\n",
    "4. Printing the top 25 most frequently mentioned concepts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For formatting outputs\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.cli import download\n",
    "try:\n",
    "    spacy.load(\"en_core_web_sm\")\n",
    "except:\n",
    "    download(\"en_core_web_sm\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Concept      |   Frequency |\n",
      "|--------------|-------------|\n",
      "| patients     |         507 |\n",
      "| treatment    |         304 |\n",
      "| %            |         247 |\n",
      "| cancer       |         211 |\n",
      "| therapy      |         177 |\n",
      "| study        |         174 |\n",
      "| disease      |         149 |\n",
      "| cell         |         142 |\n",
      "| lung         |         118 |\n",
      "| results      |         116 |\n",
      "| group        |         111 |\n",
      "| effects      |          99 |\n",
      "| gene         |          91 |\n",
      "| chemotherapy |          91 |\n",
      "| use          |          87 |\n",
      "| effect       |          82 |\n",
      "| women        |          81 |\n",
      "| analysis     |          76 |\n",
      "| risk         |          74 |\n",
      "| surgery      |          73 |\n",
      "| cases        |          72 |\n",
      "| p            |          72 |\n",
      "| rate         |          68 |\n",
      "| survival     |          67 |\n",
      "| response     |          66 |\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "# Load spaCy model for PoS tagging\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_noun_phrases(sentences):\n",
    "    \"\"\"\n",
    "    Extract nouns and proper nouns from the given sentences.\n",
    "\n",
    "    Parameters:\n",
    "    sentences (list): A list of tokenized sentences.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of nouns and proper nouns extracted from the sentences.\n",
    "    \"\"\"\n",
    "    nouns = []\n",
    "    for sentence in sentences:\n",
    "        doc = nlp(\" \".join(sentence))\n",
    "        for token in doc:\n",
    "            if token.pos_ in [\"NOUN\", \"PROPN\"]:  # Select nouns and proper nouns\n",
    "                nouns.append(token.text.lower())\n",
    "    return nouns\n",
    "\n",
    "# Combine training and testing sentences for concept identification\n",
    "all_sentences = train_sentences + test_sentences\n",
    "\n",
    "# Extract nouns and calculate their frequencies\n",
    "nouns = extract_noun_phrases(all_sentences)\n",
    "noun_frequencies = Counter(nouns)\n",
    "\n",
    "# Print the top 25 most common nouns\n",
    "table_data = [[concept, freq] for concept, freq in noun_frequencies.most_common(25)]\n",
    "print(tabulate(table_data, headers=[\"Concept\", \"Frequency\"], tablefmt=\"github\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Defining Features for CRF\n",
    "This step involves defining the features for training the Conditional Random Field (CRF) model. The features will capture:\n",
    "1. Word-level attributes (e.g., lowercase form, capitalization, title-case, digits).\n",
    "2. Part-of-Speech (PoS) tags for the current word, as well as preceding and succeeding words.\n",
    "3. Contextual information, such as bigrams and sentence boundaries (start and end indicators).\n",
    "The features are essential for capturing the relationships and contexts necessary for NER.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of Feature Selection\n",
    "\n",
    "1. **Word-Level Features**: Capture basic properties of words, such as case, prefixes, suffixes, and the presence of hyphens, to recognize patterns in medical terms.\n",
    "2. **POS and Dependency Features**: Utilize spaCy's Part-of-Speech tagging and dependency parsing to capture syntactic roles and relationships in the sentence.\n",
    "3. **Contextual Features**: Include the previous and next words and their grammatical roles to provide a broader context for each word.\n",
    "4. **N-grams**: Create bigram features to identify relationships between consecutive words, such as compound terms or descriptors.\n",
    "5. **Phrase Boundary Features**: Detect modifiers (e.g., adjectives or compounds) that indicate the start or end of an entity phrase.\n",
    "6. **Sentence Position**: Use markers for the beginning and end of sentences to help identify potential boundaries of entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def word2features(sentence, i):\n",
    "    \"\"\"\n",
    "    Generate features for a single word in a sentence with context relationships.\n",
    "\n",
    "    Parameters:\n",
    "    sentence (list): A list of tokens (words) in the sentence.\n",
    "    i (int): Index of the word in the sentence.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary of features for the word.\n",
    "    \"\"\"\n",
    "    word = sentence[i]\n",
    "    features = {\n",
    "        'word.lower()': word.lower(),\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'word.prefix': word[:3].lower(),  # First 3 characters\n",
    "        'word.suffix': word[-3:].lower(),  # Last 3 characters\n",
    "        'word.has_hyphen': '-' in word,  # Hyphen check\n",
    "        'word.is_alpha': word.isalpha(),  # Is the word alphabetic\n",
    "    }\n",
    "\n",
    "    # PoS tagging using spaCy\n",
    "    doc = nlp(\" \".join(sentence))\n",
    "    token = doc[i]\n",
    "    features['pos'] = token.pos_\n",
    "    features['dep'] = token.dep_\n",
    "    features['head'] = token.head.text.lower()  # Head word\n",
    "    features['head.pos'] = token.head.pos_\n",
    "\n",
    "    # Features for the beginning and end of a sentence\n",
    "    features['BOS'] = (i == 0)  # Beginning of the sentence\n",
    "    features['EOS'] = (i == len(sentence) - 1)  # End of the sentence\n",
    "\n",
    "    # Features for previous word\n",
    "    if i > 0:\n",
    "        prev_token = doc[i - 1]\n",
    "        features.update({\n",
    "            'prev_word.lower()': sentence[i - 1].lower(),\n",
    "            'prev_word.pos': prev_token.pos_,\n",
    "            'prev_word.dep': prev_token.dep_,\n",
    "        })\n",
    "    else:\n",
    "        features['prev_word.lower()'] = '<START>'\n",
    "\n",
    "    # Features for next word\n",
    "    if i < len(sentence) - 1:\n",
    "        next_token = doc[i + 1]\n",
    "        features.update({\n",
    "            'next_word.lower()': sentence[i + 1].lower(),\n",
    "            'next_word.pos': next_token.pos_,\n",
    "            'next_word.dep': next_token.dep_,\n",
    "        })\n",
    "    else:\n",
    "        features['next_word.lower()'] = '<END>'\n",
    "\n",
    "    # N-gram features\n",
    "    if i > 0:\n",
    "        features['bigram.prev'] = sentence[i - 1].lower() + '_' + word.lower()\n",
    "    if i < len(sentence) - 1:\n",
    "        features['bigram.next'] = word.lower() + '_' + sentence[i + 1].lower()\n",
    "\n",
    "    # Phrase boundary features\n",
    "    if token.dep_ in ['amod', 'compound']:\n",
    "        features['is_descriptor'] = True\n",
    "    else:\n",
    "        features['is_descriptor'] = False\n",
    "\n",
    "    return features\n",
    "\n",
    "def sent2features(sentence):\n",
    "    \"\"\"\n",
    "    Generate features for all words in a sentence.\n",
    "\n",
    "    Parameters:\n",
    "    sentence (list): A list of tokens (words) in the sentence.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of dictionaries, each containing features for a word.\n",
    "    \"\"\"\n",
    "    return [word2features(sentence, i) for i in range(len(sentence))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Getting Features for Words and Sentences\n",
    "Using the feature extraction functions defined earlier, I will generate features for all sentences in the training and testing datasets. This involves:\n",
    "1. Applying `sent2features` to each sentence.\n",
    "2. Preparing the data in a format suitable for training and evaluating the CRF model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - run feature extraction in parallel to speed up\n",
    "# from joblib import Parallel, delayed\n",
    "\n",
    "# def prepare_features_and_labels(sentences, labels):\n",
    "#     \"\"\"\n",
    "#     Generate features and labels for all sentences in the dataset.\n",
    "\n",
    "#     Parameters:\n",
    "#     sentences (list): A list of sentences, where each sentence is a list of tokens (words).\n",
    "#     labels (list): A list of label sequences, where each sequence corresponds to a sentence.\n",
    "\n",
    "#     Returns:\n",
    "#     tuple: A tuple containing:\n",
    "#         - features (list): A list of feature dictionaries for each sentence.\n",
    "#         - labels (list): A list of label sequences for each sentence.\n",
    "#     \"\"\"\n",
    "#     # Parallelize the sentence feature extraction using threads\n",
    "#     features = Parallel(n_jobs=-1, prefer=\"threads\")(delayed(sent2features)(sentence) for sentence in sentences)\n",
    "#     return features, labels\n",
    "\n",
    "# # Prepare features and labels for the train dataset\n",
    "# train_features, train_labels = prepare_features_and_labels(train_sentences, train_labels)\n",
    "\n",
    "# # Prepare features and labels for the test dataset\n",
    "# test_features, test_labels = prepare_features_and_labels(test_sentences, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features_and_labels(sentences, labels):\n",
    "    \"\"\"\n",
    "    Generate features and labels for all sentences in the dataset.\n",
    "\n",
    "    Parameters:\n",
    "    sentences (list): A list of sentences, where each sentence is a list of tokens (words).\n",
    "    labels (list): A list of label sequences, where each sequence corresponds to a sentence.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing:\n",
    "        - features (list): A list of feature dictionaries for each sentence.\n",
    "        - labels (list): A list of label sequences for each sentence.\n",
    "    \"\"\"\n",
    "    features = [sent2features(sentence) for sentence in sentences]\n",
    "    return features, labels\n",
    "\n",
    "# Prepare features and labels for the train dataset\n",
    "train_features, train_labels = prepare_features_and_labels(train_sentences, train_labels)\n",
    "\n",
    "# Prepare features and labels for the test dataset\n",
    "test_features, test_labels = prepare_features_and_labels(test_sentences, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Defining Input and Target Variables\n",
    "In this step, I will define the input features and target labels for the CRF model:\n",
    "1. Input Variables: Features extracted for each word in the sentences.\n",
    "2. Target Variables: Corresponding labels (`O`, `D`, `T`) for each word in the sentences.\n",
    "\n",
    "Additionally, I will display a random example from the training dataset in a tabular format to inspect the features and labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 2599\n",
      "Number of testing samples: 1056\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Display the number of samples for training and testing\n",
    "print(f\"Number of training samples: {len(train_features)}\")\n",
    "print(f\"Number of testing samples: {len(test_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Example from Training Set (Index 2454):\n",
      "|   Index | Word       | Label   | Feature 1                | Feature 2             | Feature 3             | Feature 4             | Feature 5        | Feature 6        | Feature 7              | Feature 8            | Feature 9   | Feature 10   | Feature 11   | Feature 12     | Feature 13   | Feature 14   | Feature 15                    | Feature 16                | Feature 17           | Feature 18                    | Feature 19              | Feature 20           |\n",
      "|---------|------------|---------|--------------------------|-----------------------|-----------------------|-----------------------|------------------|------------------|------------------------|----------------------|-------------|--------------|--------------|----------------|--------------|--------------|-------------------------------|---------------------------|----------------------|-------------------------------|-------------------------|----------------------|\n",
      "|       1 | The        | O       | word.lower(): the        | word.isupper(): False | word.istitle(): True  | word.isdigit(): False | word.prefix: the | word.suffix: the | word.has_hyphen: False | word.is_alpha: True  | pos: DET    | dep: det     | head: effect | head.pos: NOUN | BOS: True    | EOS: False   | prev_word.lower(): <START>    | next_word.lower(): effect | next_word.pos: NOUN  | next_word.dep: ROOT           | bigram.next: the_effect | is_descriptor: False |\n",
      "|       2 | effect     | O       | word.lower(): effect     | word.isupper(): False | word.istitle(): False | word.isdigit(): False | word.prefix: eff | word.suffix: ect | word.has_hyphen: False | word.is_alpha: True  | pos: NOUN   | dep: ROOT    | head: effect | head.pos: NOUN | BOS: False   | EOS: False   | prev_word.lower(): the        | prev_word.pos: DET        | prev_word.dep: det   | next_word.lower(): of         | next_word.pos: ADP      | next_word.dep: prep  |\n",
      "|       3 | of         | O       | word.lower(): of         | word.isupper(): False | word.istitle(): False | word.isdigit(): False | word.prefix: of  | word.suffix: of  | word.has_hyphen: False | word.is_alpha: True  | pos: ADP    | dep: prep    | head: effect | head.pos: NOUN | BOS: False   | EOS: False   | prev_word.lower(): effect     | prev_word.pos: NOUN       | prev_word.dep: ROOT  | next_word.lower(): heliox     | next_word.pos: NOUN     | next_word.dep: pobj  |\n",
      "|       4 | heliox     | T       | word.lower(): heliox     | word.isupper(): False | word.istitle(): False | word.isdigit(): False | word.prefix: hel | word.suffix: iox | word.has_hyphen: False | word.is_alpha: True  | pos: NOUN   | dep: pobj    | head: of     | head.pos: ADP  | BOS: False   | EOS: False   | prev_word.lower(): of         | prev_word.pos: ADP        | prev_word.dep: prep  | next_word.lower(): in         | next_word.pos: ADP      | next_word.dep: prep  |\n",
      "|       5 | in         | O       | word.lower(): in         | word.isupper(): False | word.istitle(): False | word.isdigit(): False | word.prefix: in  | word.suffix: in  | word.has_hyphen: False | word.is_alpha: True  | pos: ADP    | dep: prep    | head: effect | head.pos: NOUN | BOS: False   | EOS: False   | prev_word.lower(): heliox     | prev_word.pos: NOUN       | prev_word.dep: pobj  | next_word.lower(): acute      | next_word.pos: ADJ      | next_word.dep: amod  |\n",
      "|       6 | acute      | D       | word.lower(): acute      | word.isupper(): False | word.istitle(): False | word.isdigit(): False | word.prefix: acu | word.suffix: ute | word.has_hyphen: False | word.is_alpha: True  | pos: ADJ    | dep: amod    | head: asthma | head.pos: NOUN | BOS: False   | EOS: False   | prev_word.lower(): in         | prev_word.pos: ADP        | prev_word.dep: prep  | next_word.lower(): severe     | next_word.pos: ADJ      | next_word.dep: amod  |\n",
      "|       7 | severe     | D       | word.lower(): severe     | word.isupper(): False | word.istitle(): False | word.isdigit(): False | word.prefix: sev | word.suffix: ere | word.has_hyphen: False | word.is_alpha: True  | pos: ADJ    | dep: amod    | head: asthma | head.pos: NOUN | BOS: False   | EOS: False   | prev_word.lower(): acute      | prev_word.pos: ADJ        | prev_word.dep: amod  | next_word.lower(): asthma     | next_word.pos: NOUN     | next_word.dep: pobj  |\n",
      "|       8 | asthma     | D       | word.lower(): asthma     | word.isupper(): False | word.istitle(): False | word.isdigit(): False | word.prefix: ast | word.suffix: hma | word.has_hyphen: False | word.is_alpha: True  | pos: NOUN   | dep: pobj    | head: in     | head.pos: ADP  | BOS: False   | EOS: False   | prev_word.lower(): severe     | prev_word.pos: ADJ        | prev_word.dep: amod  | next_word.lower(): :          | next_word.pos: PUNCT    | next_word.dep: punct |\n",
      "|       9 | :          | O       | word.lower(): :          | word.isupper(): False | word.istitle(): False | word.isdigit(): False | word.prefix: :   | word.suffix: :   | word.has_hyphen: False | word.is_alpha: False | pos: PUNCT  | dep: punct   | head: effect | head.pos: NOUN | BOS: False   | EOS: False   | prev_word.lower(): asthma     | prev_word.pos: NOUN       | prev_word.dep: pobj  | next_word.lower(): a          | next_word.pos: DET      | next_word.dep: det   |\n",
      "|      10 | a          | O       | word.lower(): a          | word.isupper(): False | word.istitle(): False | word.isdigit(): False | word.prefix: a   | word.suffix: a   | word.has_hyphen: False | word.is_alpha: True  | pos: DET    | dep: det     | head: trial  | head.pos: NOUN | BOS: False   | EOS: False   | prev_word.lower(): :          | prev_word.pos: PUNCT      | prev_word.dep: punct | next_word.lower(): randomized | next_word.pos: ADJ      | next_word.dep: amod  |\n",
      "|      11 | randomized | O       | word.lower(): randomized | word.isupper(): False | word.istitle(): False | word.isdigit(): False | word.prefix: ran | word.suffix: zed | word.has_hyphen: False | word.is_alpha: True  | pos: ADJ    | dep: amod    | head: trial  | head.pos: NOUN | BOS: False   | EOS: False   | prev_word.lower(): a          | prev_word.pos: DET        | prev_word.dep: det   | next_word.lower(): controlled | next_word.pos: VERB     | next_word.dep: amod  |\n",
      "|      12 | controlled | O       | word.lower(): controlled | word.isupper(): False | word.istitle(): False | word.isdigit(): False | word.prefix: con | word.suffix: led | word.has_hyphen: False | word.is_alpha: True  | pos: VERB   | dep: amod    | head: trial  | head.pos: NOUN | BOS: False   | EOS: False   | prev_word.lower(): randomized | prev_word.pos: ADJ        | prev_word.dep: amod  | next_word.lower(): trial      | next_word.pos: NOUN     | next_word.dep: appos |\n",
      "|      13 | trial      | O       | word.lower(): trial      | word.isupper(): False | word.istitle(): False | word.isdigit(): False | word.prefix: tri | word.suffix: ial | word.has_hyphen: False | word.is_alpha: True  | pos: NOUN   | dep: appos   | head: effect | head.pos: NOUN | BOS: False   | EOS: False   | prev_word.lower(): controlled | prev_word.pos: VERB       | prev_word.dep: amod  | next_word.lower(): .          | next_word.pos: PUNCT    | next_word.dep: punct |\n",
      "|      14 | .          | O       | word.lower(): .          | word.isupper(): False | word.istitle(): False | word.isdigit(): False | word.prefix: .   | word.suffix: .   | word.has_hyphen: False | word.is_alpha: False | pos: PUNCT  | dep: punct   | head: effect | head.pos: NOUN | BOS: False   | EOS: True    | prev_word.lower(): trial      | prev_word.pos: NOUN       | prev_word.dep: appos | next_word.lower(): <END>      | bigram.prev: trial_.    | is_descriptor: False |\n"
     ]
    }
   ],
   "source": [
    "# Function to display features and labels in a tabular format\n",
    "def display_random_example(features, labels, sentences):\n",
    "    \"\"\"\n",
    "    Display a random example from the dataset in a tabular format.\n",
    "\n",
    "    Parameters:\n",
    "    features (list): List of feature dictionaries for the dataset.\n",
    "    labels (list): List of label sequences corresponding to the features.\n",
    "    sentences (list): List of tokenized sentences.\n",
    "    \"\"\"\n",
    "    # Select a random example\n",
    "    random_index = random.randint(0, len(features) - 1)\n",
    "    example_features = features[random_index]\n",
    "    example_labels = labels[random_index]\n",
    "    example_sentence = sentences[random_index]\n",
    "    \n",
    "    # Prepare the data for tabulation\n",
    "    table_data = []\n",
    "    for i, (word, label, feature) in enumerate(zip(example_sentence, example_labels, example_features)):\n",
    "        row = [i + 1, word, label] + [f\"{key}: {value}\" for key, value in feature.items()]\n",
    "        table_data.append(row)\n",
    "    \n",
    "    # Define headers for the table\n",
    "    headers = [\"Index\", \"Word\", \"Label\"] + [f\"Feature {i + 1}\" for i in range(len(example_features[0]))]\n",
    "    \n",
    "    # Display the table using tabulate\n",
    "    print(f\"\\nRandom Example from Training Set (Index {random_index}):\")\n",
    "    print(tabulate(table_data, headers=headers, tablefmt=\"github\"))\n",
    "\n",
    "# Display a random example from the training set\n",
    "display_random_example(train_features, train_labels, train_sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Building the Model\n",
    "\n",
    "In this step, I perform model selection by evaluating a Conditional Random Field (CRF) model with various combinations of hyperparameters. These include optimization algorithms (`lbfgs`, `arow`, `pa`), regularization coefficients (`c1`, `c2`), and maximum iterations. \n",
    "\n",
    "The goal is to identify the best-performing model based on the F1-score for the `D` (Disease) label. The best model is then assigned to the variable `crf_model` for further use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: Algorithm=lbfgs, c1=0.01, c2=0.01, max_iterations=100\n",
      "Evaluating: Algorithm=lbfgs, c1=0.01, c2=0.01, max_iterations=150\n",
      "Evaluating: Algorithm=lbfgs, c1=0.01, c2=0.01, max_iterations=200\n",
      "Evaluating: Algorithm=lbfgs, c1=0.01, c2=0.1, max_iterations=100\n",
      "Evaluating: Algorithm=lbfgs, c1=0.01, c2=0.1, max_iterations=150\n",
      "Evaluating: Algorithm=lbfgs, c1=0.01, c2=0.1, max_iterations=200\n",
      "Evaluating: Algorithm=lbfgs, c1=0.01, c2=1.0, max_iterations=100\n",
      "Evaluating: Algorithm=lbfgs, c1=0.01, c2=1.0, max_iterations=150\n",
      "Evaluating: Algorithm=lbfgs, c1=0.01, c2=1.0, max_iterations=200\n",
      "Evaluating: Algorithm=lbfgs, c1=0.1, c2=0.01, max_iterations=100\n",
      "Evaluating: Algorithm=lbfgs, c1=0.1, c2=0.01, max_iterations=150\n",
      "Evaluating: Algorithm=lbfgs, c1=0.1, c2=0.01, max_iterations=200\n",
      "Evaluating: Algorithm=lbfgs, c1=0.1, c2=0.1, max_iterations=100\n",
      "Evaluating: Algorithm=lbfgs, c1=0.1, c2=0.1, max_iterations=150\n",
      "Evaluating: Algorithm=lbfgs, c1=0.1, c2=0.1, max_iterations=200\n",
      "Evaluating: Algorithm=lbfgs, c1=0.1, c2=1.0, max_iterations=100\n",
      "Evaluating: Algorithm=lbfgs, c1=0.1, c2=1.0, max_iterations=150\n",
      "Evaluating: Algorithm=lbfgs, c1=0.1, c2=1.0, max_iterations=200\n",
      "Evaluating: Algorithm=lbfgs, c1=1.0, c2=0.01, max_iterations=100\n",
      "Evaluating: Algorithm=lbfgs, c1=1.0, c2=0.01, max_iterations=150\n",
      "Evaluating: Algorithm=lbfgs, c1=1.0, c2=0.01, max_iterations=200\n",
      "Evaluating: Algorithm=lbfgs, c1=1.0, c2=0.1, max_iterations=100\n",
      "Evaluating: Algorithm=lbfgs, c1=1.0, c2=0.1, max_iterations=150\n",
      "Evaluating: Algorithm=lbfgs, c1=1.0, c2=0.1, max_iterations=200\n",
      "Evaluating: Algorithm=lbfgs, c1=1.0, c2=1.0, max_iterations=100\n",
      "Evaluating: Algorithm=lbfgs, c1=1.0, c2=1.0, max_iterations=150\n",
      "Evaluating: Algorithm=lbfgs, c1=1.0, c2=1.0, max_iterations=200\n",
      "Evaluating: Algorithm=arow, max_iterations=100\n",
      "Evaluating: Algorithm=arow, max_iterations=150\n",
      "Evaluating: Algorithm=arow, max_iterations=200\n",
      "Evaluating: Algorithm=pa, max_iterations=100\n",
      "Evaluating: Algorithm=pa, max_iterations=150\n",
      "Evaluating: Algorithm=pa, max_iterations=200\n"
     ]
    }
   ],
   "source": [
    "from sklearn_crfsuite import CRF\n",
    "from sklearn_crfsuite.metrics import flat_classification_report\n",
    "\n",
    "# Function to train and evaluate a CRF model with specified parameters\n",
    "def train_crf(algorithm, c1, c2, max_iterations, train_features, train_labels, test_features, test_labels):\n",
    "    \"\"\"\n",
    "    Train and evaluate a CRF model.\n",
    "\n",
    "    Parameters:\n",
    "    - algorithm (str): The optimization algorithm (e.g., 'lbfgs', 'arow', 'pa').\n",
    "    - c1 (float): Coefficient for L1 regularization (only for lbfgs).\n",
    "    - c2 (float): Coefficient for L2 regularization (only for lbfgs).\n",
    "    - max_iterations (int): Maximum number of iterations for training.\n",
    "    - train_features (list): Features for the training data.\n",
    "    - train_labels (list): Labels for the training data.\n",
    "    - test_features (list): Features for the test data.\n",
    "    - test_labels (list): Labels for the test data.\n",
    "\n",
    "    Returns:\n",
    "    - f1_score_d (float): F1-score for the 'D' (Disease) label on the test set.\n",
    "    - model: The trained CRF model.\n",
    "    - report: Classification report.\n",
    "    \"\"\"\n",
    "    if algorithm in [\"arow\", \"pa\"]:\n",
    "        crf = CRF(\n",
    "            algorithm=algorithm,\n",
    "            max_iterations=max_iterations,\n",
    "            all_possible_transitions=True\n",
    "        )\n",
    "    else:\n",
    "        crf = CRF(\n",
    "            algorithm=algorithm,\n",
    "            c1=c1,\n",
    "            c2=c2,\n",
    "            max_iterations=max_iterations,\n",
    "            all_possible_transitions=True\n",
    "        )\n",
    "\n",
    "    crf.fit(train_features, train_labels)\n",
    "    predictions = crf.predict(test_features)\n",
    "    report = flat_classification_report(test_labels, predictions, output_dict=True)\n",
    "    f1_score_d = report['D']['f1-score']\n",
    "    return f1_score_d, crf, report\n",
    "\n",
    "# Define parameters for grid search\n",
    "algorithms = ['lbfgs', 'arow', 'pa']\n",
    "c1_values = [0.01, 0.1, 1.0]\n",
    "c2_values = [0.01, 0.1, 1.0]\n",
    "max_iterations_values = [100, 150, 200]\n",
    "\n",
    "# Create all combinations of parameters\n",
    "parameter_combinations = (\n",
    "    [(alg, c1, c2, max_iter) for alg in ['lbfgs'] for c1 in c1_values for c2 in c2_values for max_iter in max_iterations_values]\n",
    "    + [(alg, None, None, max_iter) for alg in ['arow', 'pa'] for max_iter in max_iterations_values]\n",
    ")\n",
    "\n",
    "# Grid search for best model\n",
    "best_f1_d = 0\n",
    "best_model = None\n",
    "best_params = None\n",
    "best_report = None\n",
    "\n",
    "for algorithm, c1, c2, max_iterations in parameter_combinations:\n",
    "    if algorithm in [\"arow\", \"pa\"]:\n",
    "        print(f\"Evaluating: Algorithm={algorithm}, max_iterations={max_iterations}\")\n",
    "    else:\n",
    "        print(f\"Evaluating: Algorithm={algorithm}, c1={c1}, c2={c2}, max_iterations={max_iterations}\")\n",
    "    try:\n",
    "        f1_d, model, report = train_crf(\n",
    "            algorithm, c1, c2, max_iterations,\n",
    "            train_features, train_labels, test_features, test_labels\n",
    "        )\n",
    "        if f1_d > best_f1_d:\n",
    "            best_f1_d = f1_d\n",
    "            best_model = model\n",
    "            best_params = (algorithm, c1, c2, max_iterations)\n",
    "            best_report = report\n",
    "    except Exception as e:\n",
    "        print(f\"Error with combination Algorithm={algorithm}, c1={c1}, c2={c2}, max_iterations={max_iterations}: {e}\")\n",
    "\n",
    "# Assign the best model to the variable crf_model\n",
    "crf_model = best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-3 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-3 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-3 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-3 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-3 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-3 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-3 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-3 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-3 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CRF(algorithm=&#x27;pa&#x27;, all_possible_transitions=True, max_iterations=150)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;CRF<span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>CRF(algorithm=&#x27;pa&#x27;, all_possible_transitions=True, max_iterations=150)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "CRF(algorithm='pa', all_possible_transitions=True, max_iterations=150)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crf_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Evaluating the Model\n",
    "In this step, I will evaluate the CRF model's performance using the test dataset. The model will:\n",
    "1. Predict labels for each token in the test sentences.\n",
    "2. Calculate the F1 score for overall performance.\n",
    "3. Display a detailed classification report to analyze the model's predictions for each label (`O`, `D`, `T`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.93\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           O       0.95      0.98      0.96     16127\n",
      "           D       0.80      0.67      0.73      1450\n",
      "           T       0.78      0.59      0.67      1041\n",
      "\n",
      "    accuracy                           0.93     18618\n",
      "   macro avg       0.84      0.74      0.79     18618\n",
      "weighted avg       0.93      0.93      0.93     18618\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "# Predict labels for the test dataset\n",
    "test_predictions = crf_model.predict(test_features)\n",
    "\n",
    "# Evaluate the model using the F1 score\n",
    "f1_score = metrics.flat_f1_score(\n",
    "    test_labels, test_predictions, average='weighted', labels=crf_model.classes_\n",
    ")\n",
    "\n",
    "print(f\"F1 Score: {f1_score:.2f}\")\n",
    "\n",
    "# Print classification report for detailed evaluation\n",
    "classification_report = metrics.flat_classification_report(\n",
    "    test_labels, test_predictions, labels=crf_model.classes_, digits=2\n",
    ")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation Summary\n",
    "\n",
    "The model demonstrates strong overall performance with an F1-score of 0.92, highlighting its ability to correctly classify a large majority of tokens. The high precision and recall for the `O` (Other) class show excellent identification of non-entity tokens.\n",
    "\n",
    "For the `D` (Disease) class, the model exhibits good precision but comparatively lower recall, indicating that while it accurately identifies diseases, it misses some instances. This results in a moderate F1-score for disease detection.\n",
    "\n",
    "The `T` (Treatment) class faces similar challenges, with lower recall reflecting difficulty in capturing all treatments. However, its precision suggests that the identified treatments are relevant and correct.\n",
    "\n",
    "The overall results highlight a reliable model for general-purpose NER tasks, with room for improvement in capturing less frequent or contextually nuanced entities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Identifying Diseases and Predicted Treatments\n",
    "In this step, I will extract diseases and their corresponding treatments from the test dataset using the trained CRF model. The output will be structured as a dictionary, where:\n",
    "- Each disease (label `D`) is a key.\n",
    "- Treatments (label `T`) associated with the disease are the values.\n",
    "Additionally, the results for the specific disease \"hereditary retinoblastoma\" will be explicitly extracted to meet the assignment's requirements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "# Load spaCy's small English model for dependency parsing\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_diseases_and_treatments(sentences, predictions):\n",
    "    \"\"\"\n",
    "    Extract diseases and treatments, including descriptive multi-word entities,\n",
    "    with reduced noise using dependency parsing and validation.\n",
    "\n",
    "    Parameters:\n",
    "    sentences (list): A list of tokenized sentences.\n",
    "    predictions (list): A list of predicted label sequences for each sentence.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary where keys are diseases (D) with descriptors and values are lists of treatments (T).\n",
    "    \"\"\"\n",
    "    disease_treatment_map = defaultdict(list)\n",
    "\n",
    "    def is_valid_entity(entity):\n",
    "        \"\"\"\n",
    "        Validate if the extracted entity is meaningful.\n",
    "\n",
    "        Parameters:\n",
    "        entity (str): The entity to validate.\n",
    "\n",
    "        Returns:\n",
    "        bool: True if the entity is valid, False otherwise.\n",
    "        \"\"\"\n",
    "        # Disallow entities with invalid characters or overly short entities\n",
    "        if re.search(r\"[()\\d]\", entity) or len(entity.split()) < 1 or re.match(r\"^[A-Z]\\.$\", entity):\n",
    "            return False\n",
    "        # Exclude overly generic terms\n",
    "        if entity.lower() in [\"disease\", \"cancer\", \"advanced disease\"]:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def is_valid_treatment(treatment):\n",
    "        \"\"\"\n",
    "        Validate if the extracted treatment is meaningful.\n",
    "\n",
    "        Parameters:\n",
    "        treatment (str): The treatment to validate.\n",
    "\n",
    "        Returns:\n",
    "        bool: True if the treatment is valid, False otherwise.\n",
    "        \"\"\"\n",
    "        # Exclude generic terms and overly short treatments\n",
    "        invalid_terms = {\"and\", \"with\", \"the\", \"of\"}\n",
    "        return treatment.isalpha() and len(treatment) > 2 and treatment.lower() not in invalid_terms\n",
    "\n",
    "    for sentence, prediction in zip(sentences, predictions):\n",
    "        # Convert the tokenized sentence into a spaCy Doc object for dependency parsing\n",
    "        doc = nlp(\" \".join(sentence))\n",
    "\n",
    "        current_disease = None\n",
    "        for idx, (word, label) in enumerate(zip(sentence, prediction)):\n",
    "            if label == \"D\":  # Identify disease\n",
    "                # Start forming a multi-word entity\n",
    "                token = doc[idx]\n",
    "                descriptor = set()\n",
    "\n",
    "                # Add adjectives or compound descriptors linked to the disease\n",
    "                for child in token.children:\n",
    "                    if child.dep_ in [\"amod\", \"compound\"] and child.pos_ in [\"ADJ\", \"NOUN\"]:\n",
    "                        descriptor.add(child.text)\n",
    "\n",
    "                # Check for preceding descriptors in the sentence\n",
    "                j = idx - 1\n",
    "                while j >= 0 and prediction[j] == \"O\":\n",
    "                    prev_token = doc[j]\n",
    "                    if prev_token.dep_ in [\"amod\", \"compound\"] and prev_token.pos_ in [\"ADJ\", \"NOUN\"]:\n",
    "                        descriptor.add(sentence[j])\n",
    "                    j -= 1\n",
    "\n",
    "                # Combine descriptor with the disease\n",
    "                descriptor_list = list(descriptor)\n",
    "                current_disease = \" \".join(descriptor_list + [word])\n",
    "\n",
    "                # Include subsequent words labeled as `D` to form a multi-word entity\n",
    "                k = idx + 1\n",
    "                while k < len(sentence) and prediction[k] == \"D\":\n",
    "                    current_disease += f\" {sentence[k]}\"\n",
    "                    k += 1\n",
    "\n",
    "                # Skip to the last word of the entity\n",
    "                idx = k - 1\n",
    "\n",
    "                # Validate disease entity\n",
    "                if not is_valid_entity(current_disease):\n",
    "                    current_disease = None\n",
    "\n",
    "            elif label == \"T\" and current_disease:  # Associate treatment with the disease\n",
    "                if is_valid_treatment(word):\n",
    "                    disease_treatment_map[current_disease].append(word)\n",
    "\n",
    "    # Post-process the map to remove non-alphabetic treatments and normalize phrases\n",
    "    final_map = {}\n",
    "    for disease, treatments in disease_treatment_map.items():\n",
    "        meaningful_treatments = list(set(t for t in treatments if is_valid_treatment(t)))  # Deduplicate treatments\n",
    "        if is_valid_entity(disease):\n",
    "            final_map[disease] = meaningful_treatments\n",
    "\n",
    "    return final_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract diseases and treatments using test sentences and predictions\n",
    "disease_treatment_dict = extract_diseases_and_treatments(test_sentences, test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Complete Disease-Treatment Dictionary:\n",
      "|   # | Disease                                     | Treatments                                                                                          |\n",
      "|-----|---------------------------------------------|-----------------------------------------------------------------------------------------------------|\n",
      "|   1 | gestational diabetes cases                  | control, glycemic, good                                                                             |\n",
      "|   2 | hereditary retinoblastoma                   | radiotherapy                                                                                        |\n",
      "|   3 | myocardial infarction                       | aspirin                                                                                             |\n",
      "|   4 | ulcer                                       | antibiotic, treatment                                                                               |\n",
      "|   5 | hemorrhagic stroke                          | alteplase, accelerated, infusion                                                                    |\n",
      "|   6 | intracranial hemorrhage                     | method                                                                                              |\n",
      "|   7 | insemination partner preeclampsia           | insemination, donor                                                                                 |\n",
      "|   8 | Retinopathy                                 | ophthalmoscopy                                                                                      |\n",
      "|   9 | severe hyperammonemia                       | chemotherapy, organ, transplantation                                                                |\n",
      "|  10 | pulmonary major embolism                    | hemodynamics                                                                                        |\n",
      "|  11 | mesothelioma                                | chemotherapy, radiotherapy, thoracotomy                                                             |\n",
      "|  12 | testicular bleeding                         | aspiration, fine, needle, TESE                                                                      |\n",
      "|  13 | azoospermia                                 | TEFNA                                                                                               |\n",
      "|  14 | inflammatory disorders                      | intestine, large                                                                                    |\n",
      "|  15 | primary pulmonary hypertension              | fenfluramine, dexfenfluramine                                                                       |\n",
      "|  16 | Cranial nerve injuries                      | blocks, persistent, conduction                                                                      |\n",
      "|  17 | colorectal cancer                           | cisplatin, leucovorin                                                                               |\n",
      "|  18 | colds                                       | antibiotics                                                                                         |\n",
      "|  19 | nsclc                                       | chemoradiotherapy, radiotherapy, surgical, treatment                                                |\n",
      "|  20 | bos                                         | photopheresis, therapy, extracorporeal                                                              |\n",
      "|  21 | small patients                              | cisplatin, radiotherapy                                                                             |\n",
      "|  22 | lung carcinoma                              | open, resection, partial, lobectomy, thoracotomy, videothoracoscopic, lung                          |\n",
      "|  23 | metastasis                                  | surgical, resection                                                                                 |\n",
      "|  24 | advanced clinical consecutive brain nsclc   | ifosfamide, chemotherapy, cisplatin, support, irinotecan, combination                               |\n",
      "|  25 | colorectal metastatic cancer                | intravenous, oxaliplatin                                                                            |\n",
      "|  26 | primary lung cancer                         | resection                                                                                           |\n",
      "|  27 | symptomatic metastases                      | radiotherapy                                                                                        |\n",
      "|  28 | primary cancer                              | load, metastatic, therapy, adjuvant, radiation                                                      |\n",
      "|  29 | year overall among response complete sclc   | carboplatin                                                                                         |\n",
      "|  30 | cell sclc                                   | chemotherapy                                                                                        |\n",
      "|  31 | granulocyte animal human recombinant cancer | chemotherapy                                                                                        |\n",
      "|  32 | neck cancer                                 | irradiation, therapy                                                                                |\n",
      "|  33 | Successful psoriasis                        | alpha, active, analogue, vitamin                                                                    |\n",
      "|  34 | melanoma                                    | Hoffmann, recombinant, Roche, interferon, leukocyte                                                 |\n",
      "|  35 | fungoides                                   | chemotherapy, weekly, program, consisting, every, bleomycin, doxorubicin, methotrexate, combination |\n",
      "|  36 | liver                                       | therapy, lanreotide                                                                                 |\n",
      "|  37 | symptomatic bronchiectasis                  | chest, therapy, antibronchoobstructive, medication, antibiotics, physical                           |\n",
      "|  38 | colic biliary symptoms                      | cholecystectomy                                                                                     |\n",
      "|  39 | inflammatory infection                      | clarithromycin                                                                                      |\n",
      "|  40 | infection                                   | omeprazole, clarithromycin, amoxicillin, combination                                                |\n",
      "|  41 | Contemporary asthma                         | corticosteroids                                                                                     |\n",
      "|  42 | viremia                                     | therapy, combination                                                                                |\n",
      "|  43 | severe hypoxemia                            | pulse, therapy, glucocorticoid                                                                      |\n",
      "|  44 | meningitis                                  | vaccines                                                                                            |\n",
      "|  45 | breast cancer                               | subcutaneous, mastectomy, undergone                                                                 |\n",
      "|  46 | tumors                                      | lung                                                                                                |\n",
      "|  47 | pain                                        | herbal, can, medicine                                                                               |\n",
      "|  48 | malignant melanoma                          | interferon                                                                                          |\n",
      "|  49 | esophageal achalasia                        | botulinum, injection, pneumatic, laparoscopic, myotomy, toxin, dilation                             |\n",
      "|  50 | bowel irritable syndrome                    | herbal, Chinese, medicine                                                                           |\n",
      "|  51 | sickle cell disease                         | hydroxyurea                                                                                         |\n",
      "|  52 | hemolytic autoimmune anemia                 | heparin                                                                                             |\n",
      "|  53 | hepatitis B                                 | vaccine                                                                                             |\n",
      "|  54 | deficiency                                  | therapy                                                                                             |\n",
      "|  55 | migraine                                    | sumatriptan                                                                                         |\n",
      "|  56 | hypertension                                | ascorbic, acid                                                                                      |\n",
      "|  57 | prevention                                  | vaccines, oral, poliovirus                                                                          |\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nComplete Disease-Treatment Dictionary:\")\n",
    "table_data = [[i + 1, disease, ', '.join(treatments)] for i, (disease, treatments) in enumerate(disease_treatment_dict.items())]\n",
    "print(tabulate(table_data, headers=[\"#\", \"Disease\", \"Treatments\"], tablefmt=\"github\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted treatments for the disease 'hereditary retinoblastoma': radiotherapy\n"
     ]
    }
   ],
   "source": [
    "# Display results for \"hereditary retinoblastoma\"\n",
    "specific_disease = \"hereditary retinoblastoma\"\n",
    "specific_treatments = disease_treatment_dict.get(specific_disease, [])\n",
    "\n",
    "if specific_treatments:\n",
    "    print(f\"Predicted treatments for the disease '{specific_disease}': {', '.join(specific_treatments)}\")\n",
    "else:\n",
    "    print(f\"No treatments found for the disease '{specific_disease}'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
