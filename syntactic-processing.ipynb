{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom NER for Identifying Diseases and Treatments\n",
    "\n",
    "This notebook implements a custom Named Entity Recognition (NER) system to identify diseases and treatments from a medical dataset. The dataset is provided in tokenized format, where each word is associated with a label:\n",
    "- `O` indicates \"Other\"\n",
    "- `D` indicates \"Disease\"\n",
    "- `T` indicates \"Treatment\"\n",
    "\n",
    "## Steps in this Notebook\n",
    "1. **Data Preprocessing:** Reconstruct sentences and labels from the tokenized dataset.\n",
    "2. **Concept Identification:** Identify key concepts in the dataset using PoS tagging.\n",
    "3. **Defining Features for CRF:** Create features for training the CRF model.\n",
    "4. **Getting Features for Words and Sentences:** Apply feature definitions to all sentences.\n",
    "5. **Defining Input and Target Variables:** Prepare input features and labels for training and testing.\n",
    "6. **Building the Model:** Train the CRF model on the training dataset.\n",
    "7. **Evaluating the Model:** Evaluate the model on the test dataset using F1 score and classification metrics.\n",
    "8. **Identifying Diseases and Predicted Treatments:** Extract relationships between diseases and treatments using the trained model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q tabulate==0.9.0 spacy==3.8.3 sklearn-crfsuite==0.5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data Preprocessing\n",
    "The dataset is provided in tokenized format, where each word is stored on a separate line, and sentences are separated by blank lines. In this step, I will:\n",
    "1. Reconstruct sentences and labels from the training and testing datasets.\n",
    "2. Count the number of sentences and labels in the processed datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to the dataset files\n",
    "train_sent_path = 'data/train_sent'\n",
    "train_label_path = 'data/train_label'\n",
    "test_sent_path = 'data/test_sent'\n",
    "test_label_path = 'data/test_label'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def filter_no_entity_statements(sentences, labels):\n",
    "    \"\"\"\n",
    "    Filters out sentences and their corresponding labels that do not contain any entity labels ('D' or 'T').\n",
    "\n",
    "    Parameters:\n",
    "    sentences (list): A list of tokenized sentences, where each sentence is a list of words.\n",
    "    labels (list): A list of label sequences, where each sequence corresponds to a sentence.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Filtered lists of sentences and labels.\n",
    "    \"\"\"\n",
    "    filtered_sentences = []\n",
    "    filtered_labels = []\n",
    "\n",
    "    for sentence, label_sequence in zip(sentences, labels):\n",
    "        # Check if the label sequence contains any 'D' or 'T' labels\n",
    "        if any(label in {'D', 'T'} for label in label_sequence):\n",
    "            filtered_sentences.append(sentence)\n",
    "            filtered_labels.append(label_sequence)\n",
    "\n",
    "    return filtered_sentences, filtered_labels\n",
    "\n",
    "def clean_sentence(sentence, labels):\n",
    "    \"\"\"\n",
    "    Cleans a tokenized sentence by normalizing, removing noise, and correcting token splits.\n",
    "    Adjusts the labels accordingly to ensure alignment with cleaned tokens.\n",
    "\n",
    "    Parameters:\n",
    "    sentence (list): A list of tokens (words) in the sentence.\n",
    "    labels (list): A list of labels corresponding to the tokens.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A cleaned list of tokens and their adjusted labels.\n",
    "    \"\"\"\n",
    "    cleaned_sentence = []\n",
    "    cleaned_labels = []\n",
    "\n",
    "    for token, label in zip(sentence, labels):\n",
    "        # Remove special characters except hyphens in compound words\n",
    "        cleaned_token = re.sub(r'[^\\w\\-]', '', token)\n",
    "\n",
    "        # Preserve meaningful hyphenated words, otherwise split on hyphen\n",
    "        if '-' in cleaned_token and len(cleaned_token.split('-')) > 1:\n",
    "            sub_tokens = cleaned_token.split('-')\n",
    "            if all(len(sub) > 1 for sub in sub_tokens):  # If all parts are meaningful\n",
    "                cleaned_sentence.append(cleaned_token)\n",
    "                cleaned_labels.append(label)\n",
    "            else:\n",
    "                for sub_token in sub_tokens:\n",
    "                    cleaned_sentence.append(sub_token)\n",
    "                    cleaned_labels.append(label)\n",
    "        else:\n",
    "            # Lowercase all tokens except acronyms or proper nouns\n",
    "            if cleaned_token.isupper() and len(cleaned_token) > 1:  # Acronyms like \"HIV\"\n",
    "                cleaned_sentence.append(cleaned_token)\n",
    "                cleaned_labels.append(label)\n",
    "            else:\n",
    "                cleaned_sentence.append(cleaned_token.lower())\n",
    "                cleaned_labels.append(label)\n",
    "\n",
    "    # Filter out empty tokens or noise tokens\n",
    "    final_sentence = []\n",
    "    final_labels = []\n",
    "    for tok, lab in zip(cleaned_sentence, cleaned_labels):\n",
    "        if tok and re.search(r'[a-zA-Z]', tok):  # Ensure valid tokens remain\n",
    "            final_sentence.append(tok)\n",
    "            final_labels.append(lab)\n",
    "\n",
    "    return final_sentence, final_labels\n",
    "\n",
    "\n",
    "def clean_dataset(sentences, labels):\n",
    "    \"\"\"\n",
    "    Cleans a dataset of tokenized sentences and adjusts their corresponding labels.\n",
    "\n",
    "    Parameters:\n",
    "    sentences (list): A list of sentences, where each sentence is a list of tokens.\n",
    "    labels (list): A list of label sequences, where each sequence corresponds to a sentence.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A cleaned list of sentences and their adjusted labels.\n",
    "    \"\"\"\n",
    "    cleaned_sentences = []\n",
    "    cleaned_labels = []\n",
    "\n",
    "    for sentence, label_sequence in zip(sentences, labels):\n",
    "        cleaned_sentence, cleaned_label = clean_sentence(sentence, label_sequence)\n",
    "        cleaned_sentences.append(cleaned_sentence)\n",
    "        cleaned_labels.append(cleaned_label)\n",
    "\n",
    "    return cleaned_sentences, cleaned_labels\n",
    "\n",
    "def process_data(file_path):\n",
    "    \"\"\"\n",
    "    Read a dataset file and reconstruct sentences or labels.\n",
    "\n",
    "    Parameters:\n",
    "    file_path (str): Path to the file containing data in tokenized format.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of sentences or labels reconstructed from the file.\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    current_sentence = []\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line == \"\":  # A blank line indicates the end of a sentence\n",
    "                if current_sentence:\n",
    "                    sentences.append(current_sentence)\n",
    "                    current_sentence = []\n",
    "            else:\n",
    "                current_sentence.append(line)\n",
    "        if current_sentence:  # Add the last sentence if the file does not end with a blank line\n",
    "            sentences.append(current_sentence)\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in train dataset: 2599\n",
      "Number of label lines in train dataset: 2599\n",
      "Number of sentences in test dataset: 1056\n",
      "Number of label lines in test dataset: 1056\n"
     ]
    }
   ],
   "source": [
    "# Process train and test datasets\n",
    "train_sentences = process_data(train_sent_path)\n",
    "train_labels = process_data(train_label_path)\n",
    "test_sentences = process_data(test_sent_path)\n",
    "test_labels = process_data(test_label_path)\n",
    "\n",
    "# Verify by printing counts\n",
    "print(f\"Number of sentences in train dataset: {len(train_sentences)}\")\n",
    "print(f\"Number of label lines in train dataset: {len(train_labels)}\")\n",
    "print(f\"Number of sentences in test dataset: {len(test_sentences)}\")\n",
    "print(f\"Number of label lines in test dataset: {len(test_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean data upfront for noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in train dataset: 2599\n",
      "Number of label lines in train dataset: 2599\n",
      "Number of sentences in test dataset: 1056\n",
      "Number of label lines in test dataset: 1056\n"
     ]
    }
   ],
   "source": [
    "# train_sentences, train_labels = clean_dataset(train_sentences, train_labels)\n",
    "# test_sentences, test_labels = clean_dataset(test_sentences, test_labels)\n",
    "\n",
    "# train_sentences, train_labels = filter_no_entity_statements(sentences=train_sentences, labels=train_labels)\n",
    "# test_sentences, test_labels = filter_no_entity_statements(sentences=test_sentences, labels=test_labels)\n",
    "\n",
    "# Verify by printing counts\n",
    "print(f\"Number of sentences in train dataset: {len(train_sentences)}\")\n",
    "print(f\"Number of label lines in train dataset: {len(train_labels)}\")\n",
    "\n",
    "print(f\"Number of sentences in test dataset: {len(test_sentences)}\")\n",
    "print(f\"Number of label lines in test dataset: {len(test_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE : I concluded on not cleaning any data upfront as all attempts were detremental to model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence # 381\n",
      "[O]The [O]women [O]presented [O]with [D]cardiovascular [D]symptoms [O]or [O]a [D]heart [D]murmur\n",
      "\n",
      "Sentence # 2436\n",
      "[T]Surgical [T]treatment [O]for [D]lung [D]hydatid [D]disease\n",
      "\n",
      "Sentence # 1489\n",
      "[O]Breastfeeding [O]and [O]catch-up [O]growth [O]in [O]infants [O]born [O]small [O]for [O]gestational [O]age\n",
      "\n",
      "Sentence # 1738\n",
      "[D]Vascular [D]Parkinson [D]syndromes [O]: [O]a [O]controversial [O]concept\n",
      "\n",
      "Sentence # 2271\n",
      "[O]Refugees [O]with [D]crawling [D]lice [O]were [O]treated [O]with [O]a [T]pediculicide [T]containing [T]1 [T]% [T]permethrin\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def print_sentences_with_labels(sentences, labels, num_sentences=5):\n",
    "    \"\"\"\n",
    "    Prints a specified number of random sentences along with their labels.\n",
    "\n",
    "    Parameters:\n",
    "    sentences (list): List of tokenized sentences.\n",
    "    labels (list): List of label sequences corresponding to the sentences.\n",
    "    num_samples (int): Number of random samples to print. Default is 5.\n",
    "    \"\"\"\n",
    "    assert len(sentences) == len(labels), \"Sentences and labels must have the same length.\"\n",
    "    \n",
    "    random_indices = random.sample(range(len(sentences)), num_sentences)\n",
    "    for idx in random_indices:\n",
    "        sentence = sentences[idx]\n",
    "        label_sequence = labels[idx]\n",
    "        formatted_output = \" \".join([f\"[{label}]{word}\" for word, label in zip(sentence, label_sequence)])\n",
    "        print (f'Sentence # {idx}')\n",
    "        print(formatted_output)\n",
    "        print()\n",
    "\n",
    "print_sentences_with_labels(train_sentences, train_labels, num_sentences=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Concept Identification\n",
    "In this step, I will identify key concepts (e.g., diseases and treatments) from the dataset by:\n",
    "1. Performing Part-of-Speech (PoS) tagging on the text data.\n",
    "2. Extracting tokens with PoS tags corresponding to nouns (`NOUN` and `PROPN`).\n",
    "3. Counting the frequency of these tokens across the entire dataset (both training and testing data).\n",
    "4. Printing the top 25 most frequently mentioned concepts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For formatting outputs\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.cli import download\n",
    "try:\n",
    "    spacy.load(\"en_core_web_sm\")\n",
    "except:\n",
    "    download(\"en_core_web_sm\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "# Load spaCy model for PoS tagging\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_noun_phrases(sentences):\n",
    "    \"\"\"\n",
    "    Extract nouns and proper nouns from the given sentences.\n",
    "\n",
    "    Parameters:\n",
    "    sentences (list): A list of tokenized sentences.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of nouns and proper nouns extracted from the sentences.\n",
    "    \"\"\"\n",
    "    nouns = []\n",
    "    for sentence in sentences:\n",
    "        doc = nlp(\" \".join(sentence))\n",
    "        for token in doc:\n",
    "            if token.pos_ in [\"NOUN\", \"PROPN\"]:  # Select nouns and proper nouns\n",
    "                nouns.append(token.text.lower())\n",
    "    return nouns\n",
    "\n",
    "# Combine training and testing sentences for concept identification\n",
    "all_sentences = train_sentences + test_sentences\n",
    "\n",
    "# Extract nouns and calculate their frequencies\n",
    "nouns = extract_noun_phrases(all_sentences)\n",
    "noun_frequencies = Counter(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   # | Concept      |   Frequency |\n",
      "|-----|--------------|-------------|\n",
      "|   1 | patients     |         507 |\n",
      "|   2 | treatment    |         304 |\n",
      "|   3 | %            |         247 |\n",
      "|   4 | cancer       |         211 |\n",
      "|   5 | therapy      |         177 |\n",
      "|   6 | study        |         174 |\n",
      "|   7 | disease      |         149 |\n",
      "|   8 | cell         |         142 |\n",
      "|   9 | lung         |         118 |\n",
      "|  10 | results      |         116 |\n",
      "|  11 | group        |         111 |\n",
      "|  12 | effects      |          99 |\n",
      "|  13 | gene         |          91 |\n",
      "|  14 | chemotherapy |          91 |\n",
      "|  15 | use          |          87 |\n",
      "|  16 | effect       |          82 |\n",
      "|  17 | women        |          81 |\n",
      "|  18 | analysis     |          76 |\n",
      "|  19 | risk         |          74 |\n",
      "|  20 | surgery      |          73 |\n",
      "|  21 | cases        |          72 |\n",
      "|  22 | p            |          72 |\n",
      "|  23 | rate         |          68 |\n",
      "|  24 | survival     |          67 |\n",
      "|  25 | response     |          66 |\n"
     ]
    }
   ],
   "source": [
    "# Print the top 25 most common nouns\n",
    "table_data = [[i, concept, freq] for i, (concept, freq) in enumerate(noun_frequencies.most_common(25), start=1)]\n",
    "print(tabulate(table_data, headers=['#', \"Concept\", \"Frequency\"], tablefmt=\"github\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   # | Concept           |   Frequency |\n",
      "|-----|-------------------|-------------|\n",
      "|   1 | abortion          |           1 |\n",
      "|   2 | myeloma           |           1 |\n",
      "|   3 | tandem            |           1 |\n",
      "|   4 | occlusions        |           1 |\n",
      "|   5 | thrombogenicity   |           1 |\n",
      "|   6 | vasoreactivity    |           1 |\n",
      "|   7 | epoetin           |           1 |\n",
      "|   8 | timolol           |           1 |\n",
      "|   9 | tartrate          |           1 |\n",
      "|  10 | brimonidine       |           1 |\n",
      "|  11 | poliovirus        |           1 |\n",
      "|  12 | poliomyelitis     |           1 |\n",
      "|  13 | celecoxib         |           1 |\n",
      "|  14 | formoterol        |           1 |\n",
      "|  15 | dry               |           1 |\n",
      "|  16 | levodopa          |           1 |\n",
      "|  17 | methyltransferase |           1 |\n",
      "|  18 | catechol          |           1 |\n",
      "|  19 | tolcapone         |           1 |\n",
      "|  20 | colonoscopy       |           1 |\n",
      "|  21 | malathion         |           1 |\n",
      "|  22 | spoon             |           1 |\n",
      "|  23 | thera             |           1 |\n",
      "|  24 | ascorbic          |           1 |\n",
      "|  25 | bisoprolol        |           1 |\n"
     ]
    }
   ],
   "source": [
    "# Print the top 25 least common nouns\n",
    "table_data = [[i, concept, freq] for i, (concept, freq) in enumerate(noun_frequencies.most_common()[:-26:-1], start=1)]\n",
    "print(tabulate(table_data, headers=['#', \"Concept\", \"Frequency\"], tablefmt=\"github\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Defining Features for CRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of Feature Selection:\n",
    "\n",
    "1. **Word-Level Features**: Capture fundamental properties of words, such as their lowercased form, prefixes, and suffixes, to identify linguistic patterns.\n",
    "\n",
    "2. **POS and Dependency Features**: Leverage spaCy's Part-of-Speech tagging and dependency parsing to understand syntactic roles and relationships within the sentence.\n",
    "\n",
    "3. **Contextual Features**: Incorporate information about preceding and following words, including their grammatical roles, to provide a comprehensive context for each word.\n",
    "\n",
    "4. **N-grams**: Include both bigrams and trigrams to capture sequential patterns and relationships across consecutive words, crucial for identifying compound terms and multi-word entities.\n",
    "\n",
    "5. **Phrase Boundary Features**: Detect syntactic indicators, such as adjectives and compounds, that suggest the beginning or continuation of an entity phrase.\n",
    "\n",
    "6. **Sentence Position Indicators**: Use `BOS` (Beginning of Sentence) and `EOS` (End of Sentence) flags to capture word position within the sentence, aiding in boundary detection for entities.\n",
    "\n",
    "7. **Placeholder Features**: Handle missing or inapplicable features consistently with placeholders (e.g., `<NONE>`, `<START>`, `<END>`) to maintain uniformity across feature dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sentence, i):\n",
    "    \"\"\"\n",
    "    Generate features for a single word in a sentence with consistent context relationships.\n",
    "    \n",
    "    Parameters:\n",
    "    sentence (list): A list of tokens (words) in the sentence.\n",
    "    i (int): Index of the word in the sentence.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary of features for the word.\n",
    "    \"\"\"\n",
    "    word = sentence[i]\n",
    "    features = {\n",
    "        'word.lower()': word.lower(),\n",
    "        'word.prefix': word[:3].lower(),\n",
    "        'word.suffix': word[-3:].lower(),\n",
    "    }\n",
    "\n",
    "    # PoS tagging using spaCy\n",
    "    doc = nlp(\" \".join(sentence))\n",
    "    token = doc[i]\n",
    "    features['pos'] = token.pos_\n",
    "    features['dep'] = token.dep_\n",
    "    features['head'] = token.head.text.lower()\n",
    "    features['head.pos'] = token.head.pos_\n",
    "\n",
    "    # Previous word features\n",
    "    if i > 0:\n",
    "        prev_token = doc[i - 1]\n",
    "        features.update({\n",
    "            'prev_word.lower()': sentence[i - 1].lower(),\n",
    "            'prev_word.pos': prev_token.pos_,\n",
    "            'prev_word.dep': prev_token.dep_,\n",
    "        })\n",
    "        features['bigram.prev'] = sentence[i - 1].lower() + '_' + word.lower()\n",
    "    else:\n",
    "        features.update({\n",
    "            'prev_word.lower()': '<START>',\n",
    "            'prev_word.pos': '<NONE>',\n",
    "            'prev_word.dep': '<NONE>',\n",
    "            'bigram.prev': '<NONE>',\n",
    "        })\n",
    "\n",
    "    # Next word features\n",
    "    if i < len(sentence) - 1:\n",
    "        next_token = doc[i + 1]\n",
    "        features.update({\n",
    "            'next_word.lower()': sentence[i + 1].lower(),\n",
    "            'next_word.pos': next_token.pos_,\n",
    "            'next_word.dep': next_token.dep_,\n",
    "        })\n",
    "        features['bigram.next'] = word.lower() + '_' + sentence[i + 1].lower()\n",
    "    else:\n",
    "        features.update({\n",
    "            'next_word.lower()': '<END>',\n",
    "            'next_word.pos': '<NONE>',\n",
    "            'next_word.dep': '<NONE>',\n",
    "            'bigram.next': '<NONE>',\n",
    "        })\n",
    "\n",
    "    # Trigram features\n",
    "    if i > 1:\n",
    "        features['trigram.prev'] = sentence[i - 2].lower() + '_' + sentence[i - 1].lower() + '_' + word.lower()\n",
    "    else:\n",
    "        features['trigram.prev'] = '<NONE>'\n",
    "    if i < len(sentence) - 2:\n",
    "        features['trigram.next'] = word.lower() + '_' + sentence[i + 1].lower() + '_' + sentence[i + 2].lower()\n",
    "    else:\n",
    "        features['trigram.next'] = '<NONE>'\n",
    "\n",
    "    # Phrase boundary features\n",
    "    features['is_descriptor'] = token.dep_ in ['amod', 'compound']\n",
    "\n",
    "    # Sentence boundary features\n",
    "    features['BOS'] = (i == 0)\n",
    "    features['EOS'] = (i == len(sentence) - 1)\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def sent2features(sentence):\n",
    "    \"\"\"\n",
    "    Generate features for all words in a sentence.\n",
    "\n",
    "    Parameters:\n",
    "    sentence (list): A list of tokens (words) in the sentence.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of dictionaries, each containing features for a word.\n",
    "    \"\"\"\n",
    "    return [word2features(sentence, i) for i in range(len(sentence))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Getting Features for Words and Sentences\n",
    "Using the feature extraction functions defined earlier, I will generate features for all sentences in the training and testing datasets. This involves:\n",
    "1. Applying `sent2features` to each sentence.\n",
    "2. Preparing the data in a format suitable for training and evaluating the CRF model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - run feature extraction in parallel to speed up\n",
    "# from joblib import Parallel, delayed\n",
    "\n",
    "# def prepare_features_and_labels(sentences, labels):\n",
    "#     \"\"\"\n",
    "#     Generate features and labels for all sentences in the dataset.\n",
    "\n",
    "#     Parameters:\n",
    "#     sentences (list): A list of sentences, where each sentence is a list of tokens (words).\n",
    "#     labels (list): A list of label sequences, where each sequence corresponds to a sentence.\n",
    "\n",
    "#     Returns:\n",
    "#     tuple: A tuple containing:\n",
    "#         - features (list): A list of feature dictionaries for each sentence.\n",
    "#         - labels (list): A list of label sequences for each sentence.\n",
    "#     \"\"\"\n",
    "#     # Parallelize the sentence feature extraction using threads\n",
    "#     features = Parallel(n_jobs=-1, prefer=\"threads\")(delayed(sent2features)(sentence) for sentence in sentences)\n",
    "#     return features, labels\n",
    "\n",
    "# # Prepare features and labels for the train dataset\n",
    "# train_features, train_labels = prepare_features_and_labels(train_sentences, train_labels)\n",
    "\n",
    "# # Prepare features and labels for the test dataset\n",
    "# test_features, test_labels = prepare_features_and_labels(test_sentences, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features_and_labels(sentences, labels):\n",
    "    \"\"\"\n",
    "    Generate features and labels for all sentences in the dataset.\n",
    "\n",
    "    Parameters:\n",
    "    sentences (list): A list of sentences, where each sentence is a list of tokens (words).\n",
    "    labels (list): A list of label sequences, where each sequence corresponds to a sentence.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing:\n",
    "        - features (list): A list of feature dictionaries for each sentence.\n",
    "        - labels (list): A list of label sequences for each sentence.\n",
    "    \"\"\"\n",
    "    features = [sent2features(sentence) for sentence in sentences]\n",
    "    return features, labels\n",
    "\n",
    "# Prepare features and labels for the train dataset\n",
    "train_features, train_labels = prepare_features_and_labels(train_sentences, train_labels)\n",
    "\n",
    "# Prepare features and labels for the test dataset\n",
    "test_features, test_labels = prepare_features_and_labels(test_sentences, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Defining Input and Target Variables\n",
    "In this step, I will define the input features and target labels for the CRF model:\n",
    "1. Input Variables: Features extracted for each word in the sentences.\n",
    "2. Target Variables: Corresponding labels (`O`, `D`, `T`) for each word in the sentences.\n",
    "\n",
    "Additionally, I will display a random example from the training dataset in a tabular format to inspect the features and labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 2599\n",
      "Number of testing samples: 1056\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Display the number of samples for training and testing\n",
    "print(f\"Number of training samples: {len(train_features)}\")\n",
    "print(f\"Number of testing samples: {len(test_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Example from Training Set (Index 1228):\n",
      "|   Index | Word          | Label   | Feature 1                   | Feature 2        | Feature 3        | Feature 4   | Feature 5     | Feature 6        | Feature 7      | Feature 8                        | Feature 9             | Feature 10              | Feature 11                    | Feature 12                       | Feature 13            | Feature 14              | Feature 15                    | Feature 16                                | Feature 17                                | Feature 18           | Feature 19   | Feature 20   |\n",
      "|---------|---------------|---------|-----------------------------|------------------|------------------|-------------|---------------|------------------|----------------|----------------------------------|-----------------------|-------------------------|-------------------------------|----------------------------------|-----------------------|-------------------------|-------------------------------|-------------------------------------------|-------------------------------------------|----------------------|--------------|--------------|\n",
      "|       1 | State         | O       | word.lower(): state         | word.prefix: sta | word.suffix: ate | pos: NOUN   | dep: compound | head: regulation | head.pos: NOUN | prev_word.lower(): <START>       | prev_word.pos: <NONE> | prev_word.dep: <NONE>   | bigram.prev: <NONE>           | next_word.lower(): regulation    | next_word.pos: NOUN   | next_word.dep: ROOT     | bigram.next: state_regulation | trigram.prev: <NONE>                      | trigram.next: state_regulation_in         | is_descriptor: True  | BOS: True    | EOS: False   |\n",
      "|       2 | regulation    | O       | word.lower(): regulation    | word.prefix: reg | word.suffix: ion | pos: NOUN   | dep: ROOT     | head: regulation | head.pos: NOUN | prev_word.lower(): state         | prev_word.pos: NOUN   | prev_word.dep: compound | bigram.prev: state_regulation | next_word.lower(): in            | next_word.pos: ADP    | next_word.dep: prep     | bigram.next: regulation_in    | trigram.prev: <NONE>                      | trigram.next: regulation_in_a             | is_descriptor: False | BOS: False   | EOS: False   |\n",
      "|       3 | in            | O       | word.lower(): in            | word.prefix: in  | word.suffix: in  | pos: ADP    | dep: prep     | head: regulation | head.pos: NOUN | prev_word.lower(): regulation    | prev_word.pos: NOUN   | prev_word.dep: ROOT     | bigram.prev: regulation_in    | next_word.lower(): a             | next_word.pos: DET    | next_word.dep: det      | bigram.next: in_a             | trigram.prev: state_regulation_in         | trigram.next: in_a_world                  | is_descriptor: False | BOS: False   | EOS: False   |\n",
      "|       4 | a             | O       | word.lower(): a             | word.prefix: a   | word.suffix: a   | pos: DET    | dep: det      | head: world      | head.pos: NOUN | prev_word.lower(): in            | prev_word.pos: ADP    | prev_word.dep: prep     | bigram.prev: in_a             | next_word.lower(): world         | next_word.pos: NOUN   | next_word.dep: pobj     | bigram.next: a_world          | trigram.prev: regulation_in_a             | trigram.next: a_world_of                  | is_descriptor: False | BOS: False   | EOS: False   |\n",
      "|       5 | world         | O       | word.lower(): world         | word.prefix: wor | word.suffix: rld | pos: NOUN   | dep: pobj     | head: in         | head.pos: ADP  | prev_word.lower(): a             | prev_word.pos: DET    | prev_word.dep: det      | bigram.prev: a_world          | next_word.lower(): of            | next_word.pos: ADP    | next_word.dep: prep     | bigram.next: world_of         | trigram.prev: in_a_world                  | trigram.next: world_of_``                 | is_descriptor: False | BOS: False   | EOS: False   |\n",
      "|       6 | of            | O       | word.lower(): of            | word.prefix: of  | word.suffix: of  | pos: ADP    | dep: prep     | head: world      | head.pos: NOUN | prev_word.lower(): world         | prev_word.pos: NOUN   | prev_word.dep: pobj     | bigram.prev: world_of         | next_word.lower(): ``            | next_word.pos: PUNCT  | next_word.dep: punct    | bigram.next: of_``            | trigram.prev: a_world_of                  | trigram.next: of_``_boundary-less         | is_descriptor: False | BOS: False   | EOS: False   |\n",
      "|       7 | ``            | O       | word.lower(): ``            | word.prefix: ``  | word.suffix: ``  | pos: PUNCT  | dep: punct    | head: of         | head.pos: ADP  | prev_word.lower(): of            | prev_word.pos: ADP    | prev_word.dep: prep     | bigram.prev: of_``            | next_word.lower(): boundary-less | next_word.pos: PUNCT  | next_word.dep: punct    | bigram.next: ``_boundary-less | trigram.prev: world_of_``                 | trigram.next: ``_boundary-less_''         | is_descriptor: False | BOS: False   | EOS: False   |\n",
      "|       8 | boundary-less | O       | word.lower(): boundary-less | word.prefix: bou | word.suffix: ess | pos: PUNCT  | dep: punct    | head: of         | head.pos: ADP  | prev_word.lower(): ``            | prev_word.pos: PUNCT  | prev_word.dep: punct    | bigram.prev: ``_boundary-less | next_word.lower(): ''            | next_word.pos: NOUN   | next_word.dep: npadvmod | bigram.next: boundary-less_'' | trigram.prev: of_``_boundary-less         | trigram.next: boundary-less_''_technology | is_descriptor: False | BOS: False   | EOS: False   |\n",
      "|       9 | ''            | O       | word.lower(): ''            | word.prefix: ''  | word.suffix: ''  | pos: NOUN   | dep: npadvmod | head: less       | head.pos: ADJ  | prev_word.lower(): boundary-less | prev_word.pos: PUNCT  | prev_word.dep: punct    | bigram.prev: boundary-less_'' | next_word.lower(): technology    | next_word.pos: PUNCT  | next_word.dep: punct    | bigram.next: ''_technology    | trigram.prev: ``_boundary-less_''         | trigram.next: <NONE>                      | is_descriptor: False | BOS: False   | EOS: False   |\n",
      "|      10 | technology    | O       | word.lower(): technology    | word.prefix: tec | word.suffix: ogy | pos: PUNCT  | dep: punct    | head: less       | head.pos: ADJ  | prev_word.lower(): ''            | prev_word.pos: NOUN   | prev_word.dep: npadvmod | bigram.prev: ''_technology    | next_word.lower(): <END>         | next_word.pos: <NONE> | next_word.dep: <NONE>   | bigram.next: <NONE>           | trigram.prev: boundary-less_''_technology | trigram.next: <NONE>                      | is_descriptor: False | BOS: False   | EOS: True    |\n"
     ]
    }
   ],
   "source": [
    "# Function to display features and labels in a tabular format\n",
    "def display_random_example(features, labels, sentences):\n",
    "    \"\"\"\n",
    "    Display a random example from the dataset in a tabular format.\n",
    "\n",
    "    Parameters:\n",
    "    features (list): List of feature dictionaries for the dataset.\n",
    "    labels (list): List of label sequences corresponding to the features.\n",
    "    sentences (list): List of tokenized sentences.\n",
    "    \"\"\"\n",
    "    # Select a random example\n",
    "    random_index = random.randint(0, len(features) - 1)\n",
    "    example_features = features[random_index]\n",
    "    example_labels = labels[random_index]\n",
    "    example_sentence = sentences[random_index]\n",
    "    \n",
    "    # Prepare the data for tabulation\n",
    "    table_data = []\n",
    "    for i, (word, label, feature) in enumerate(zip(example_sentence, example_labels, example_features)):\n",
    "        row = [i + 1, word, label] + [f\"{key}: {value}\" for key, value in feature.items()]\n",
    "        table_data.append(row)\n",
    "    \n",
    "    # Define headers for the table\n",
    "    headers = [\"Index\", \"Word\", \"Label\"] + [f\"Feature {i + 1}\" for i in range(len(example_features[0]))]\n",
    "\n",
    "    # Display the table using tabulate\n",
    "    print(f\"\\nRandom Example from Training Set (Index {random_index}):\")\n",
    "    print(tabulate(table_data, headers=headers, tablefmt=\"github\"))\n",
    "\n",
    "# Display a random example from the training set\n",
    "display_random_example(train_features, train_labels, train_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Building the Model\n",
    "\n",
    "In this step, I perform model selection by evaluating a Conditional Random Field (CRF) model with various combinations of hyperparameters. These include optimization algorithms (`lbfgs`, `arow`, `pa`), regularization coefficients (`c1`, `c2`), and maximum iterations. \n",
    "\n",
    "The goal is to identify the best-performing model based on the F1-score for the `D` (Disease) label. The best model is then assigned to the variable `crf_model` for further use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am not evaluating `lbfgs` as I already tested it to conclude it is not a candidate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn_crfsuite import CRF\n",
    "from sklearn_crfsuite.metrics import flat_classification_report\n",
    "\n",
    "# Function to train and evaluate a CRF model with specified parameters\n",
    "def train_crf(algorithm, c1, c2, max_iterations, train_features, train_labels, test_features, test_labels):\n",
    "    \"\"\"\n",
    "    Train and evaluate a CRF model.\n",
    "\n",
    "    Parameters:\n",
    "    - algorithm (str): The optimization algorithm (e.g., 'lbfgs', 'arow', 'pa').\n",
    "    - c1 (float): Coefficient for L1 regularization (only for lbfgs).\n",
    "    - c2 (float): Coefficient for L2 regularization (only for lbfgs).\n",
    "    - max_iterations (int): Maximum number of iterations for training.\n",
    "    - train_features (list): Features for the training data.\n",
    "    - train_labels (list): Labels for the training data.\n",
    "    - test_features (list): Features for the test data.\n",
    "    - test_labels (list): Labels for the test data.\n",
    "\n",
    "    Returns:\n",
    "    - f1_score_d (float): F1-score for the 'D' (Disease) label on the test set.\n",
    "    - model: The trained CRF model.\n",
    "    - report: Classification report.\n",
    "    \"\"\"\n",
    "    if algorithm in [\"arow\", \"pa\"]:\n",
    "        crf = CRF(\n",
    "            algorithm=algorithm,\n",
    "            max_iterations=max_iterations,\n",
    "            all_possible_transitions=True\n",
    "        )\n",
    "    else:\n",
    "        crf = CRF(\n",
    "            algorithm=algorithm,\n",
    "            c1=c1,\n",
    "            c2=c2,\n",
    "            max_iterations=max_iterations,\n",
    "            all_possible_transitions=True\n",
    "        )\n",
    "\n",
    "    crf.fit(X=train_features, y=train_labels)\n",
    "    predictions = crf.predict(test_features)\n",
    "    report = flat_classification_report(test_labels, predictions, output_dict=True)\n",
    "    f1_score_d = report['D']['f1-score']\n",
    "    return f1_score_d, crf, report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: Algorithm=arow, max_iterations=100\n",
      "Evaluating: Algorithm=arow, max_iterations=125\n",
      "Evaluating: Algorithm=arow, max_iterations=150\n",
      "Evaluating: Algorithm=arow, max_iterations=175\n",
      "Evaluating: Algorithm=arow, max_iterations=200\n",
      "Evaluating: Algorithm=arow, max_iterations=250\n",
      "Evaluating: Algorithm=arow, max_iterations=300\n",
      "Evaluating: Algorithm=pa, max_iterations=100\n",
      "Evaluating: Algorithm=pa, max_iterations=125\n",
      "Evaluating: Algorithm=pa, max_iterations=150\n",
      "Evaluating: Algorithm=pa, max_iterations=175\n",
      "Evaluating: Algorithm=pa, max_iterations=200\n",
      "Evaluating: Algorithm=pa, max_iterations=250\n",
      "Evaluating: Algorithm=pa, max_iterations=300\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-9 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-9 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-9 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-9 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-9 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-9 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-9 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-9 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-9 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-9 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-9 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-9 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-9 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-9 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-9\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CRF(algorithm=&#x27;pa&#x27;, all_possible_transitions=True, max_iterations=150)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" checked><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;CRF<span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>CRF(algorithm=&#x27;pa&#x27;, all_possible_transitions=True, max_iterations=150)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "CRF(algorithm='pa', all_possible_transitions=True, max_iterations=150)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define parameters for grid search\n",
    "algorithms = ['arow', 'pa']\n",
    "max_iterations_values = [100, 125, 150, 175, 200, 250, 300]\n",
    "\n",
    "parameter_combinations = (\n",
    "    [(alg, None, None, max_iter) for alg in ['arow', 'pa'] for max_iter in max_iterations_values]\n",
    ")\n",
    "\n",
    "# Grid search for best model\n",
    "best_f1_d = 0\n",
    "best_model = None\n",
    "best_params = None\n",
    "best_report = None\n",
    "\n",
    "for algorithm, c1, c2, max_iterations in parameter_combinations:\n",
    "    if algorithm in [\"arow\", \"pa\"]:\n",
    "        print(f\"Evaluating: Algorithm={algorithm}, max_iterations={max_iterations}\")\n",
    "    # else:\n",
    "    #     print(f\"Evaluating: Algorithm={algorithm}, c1={c1}, c2={c2}, max_iterations={max_iterations}\")\n",
    "    try:\n",
    "        f1_d, model, report = train_crf(\n",
    "            algorithm, c1, c2, max_iterations,\n",
    "            train_features, train_labels, test_features, test_labels\n",
    "        )\n",
    "        if f1_d > best_f1_d:\n",
    "            best_f1_d = f1_d\n",
    "            best_model = model\n",
    "            best_params = (algorithm, c1, c2, max_iterations)\n",
    "            best_report = report\n",
    "    except Exception as e:\n",
    "        print(f\"Error with combination Algorithm={algorithm}, c1={c1}, c2={c2}, max_iterations={max_iterations}: {e}\")\n",
    "\n",
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-12 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-12 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-12 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-12 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-12 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-12 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-12 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-12 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-12 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-12 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-12 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-12 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-12 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-12 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-12 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-12 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-12 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-12 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-12 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-12 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-12 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-12 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-12 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-12 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-12 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-12 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-12 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-12 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-12 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-12 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-12 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-12 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-12 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-12 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-12 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-12 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-12 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-12 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-12 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-12 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-12 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-12 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-12\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CRF(algorithm=&#x27;pa&#x27;, all_possible_transitions=True, max_iterations=150)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" checked><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;CRF<span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>CRF(algorithm=&#x27;pa&#x27;, all_possible_transitions=True, max_iterations=150)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "CRF(algorithm='pa', all_possible_transitions=True, max_iterations=150)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hard coded to avoid confusion \n",
    "crf_model = CRF(\n",
    "            algorithm='pa',\n",
    "            max_iterations=150,\n",
    "            all_possible_transitions=True\n",
    ")\n",
    "\n",
    "crf_model.fit(X=train_features, y=train_labels)\n",
    "crf_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Evaluating the Model\n",
    "In this step, I will evaluate the CRF model's performance using the test dataset. The model will:\n",
    "1. Predict labels for each token in the test sentences.\n",
    "2. Calculate the F1 score for overall performance.\n",
    "3. Display a detailed classification report to analyze the model's predictions for each label (`O`, `D`, `T`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           O       0.94      0.98      0.96     16127\n",
      "           D       0.79      0.64      0.71      1450\n",
      "           T       0.80      0.56      0.66      1041\n",
      "\n",
      "    accuracy                           0.93     18618\n",
      "   macro avg       0.85      0.73      0.78     18618\n",
      "weighted avg       0.92      0.93      0.92     18618\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "# Predict labels for the test dataset\n",
    "test_predictions = crf_model.predict(test_features)\n",
    "\n",
    "# Evaluate the model using the F1 score\n",
    "# f1_score = metrics.flat_f1_score(\n",
    "#     test_labels, test_predictions, average='weighted', labels=crf_model.classes_\n",
    "# )\n",
    "\n",
    "# print(f\"F1 Score: {f1_score:.3f}\")\n",
    "\n",
    "# Print classification report for detailed evaluation\n",
    "classification_report = metrics.flat_classification_report(\n",
    "    test_labels, test_predictions, labels=crf_model.classes_, digits=2\n",
    ")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model Performance Summary**\n",
    "\n",
    "#### **Overall Performance**\n",
    "- **Accuracy**: **0.93**\n",
    "- **Macro Average F1-Score**: **0.78**\n",
    "- **Weighted Average F1-Score**: **0.93**\n",
    "\n",
    "The model demonstrates strong overall performance, especially for non-entity tokens (`O`), while maintaining moderate performance for entities (`D` and `T`). There is room for improvement in recall for minority classes.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Key Observations**\n",
    "1. **Strengths**:\n",
    "   - Outstanding performance for the `O` class, indicating strong handling of non-entity tokens.\n",
    "   - High precision for `D` and `T` classes minimizes false positives.\n",
    "\n",
    "2. **Weaknesses**:\n",
    "   - Recall for `D` and `T` classes remains suboptimal, leading to missed entities.\n",
    "   - The lower macro F1-score (**0.78**) highlights the variability in performance between classes.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Conclusion**\n",
    "The model excels in detecting non-entity tokens and maintains good precision for entity classes. However, further improvements in recall for the `D` and `T` classes are necessary to enhance overall entity detection. Refining features or augmenting the training dataset may help address these challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Identifying Diseases and Predicted Treatments\n",
    "In this step, I will extract diseases and their corresponding treatments from the test dataset using the trained CRF model. The output will be structured as a dictionary, where:\n",
    "- Each disease (label `D`) is a key.\n",
    "- Treatments (label `T`) associated with the disease are the values.\n",
    "Additionally, the results for the specific disease \"hereditary retinoblastoma\" will be explicitly extracted to meet the assignment's requirements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "# Load spaCy's small English model for dependency parsing\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_diseases_and_treatments(sentences, predictions):\n",
    "    \"\"\"\n",
    "    Extract diseases and treatments, including descriptive multi-word entities,\n",
    "    with reduced noise using dependency parsing and validation.\n",
    "\n",
    "    Parameters:\n",
    "    sentences (list): A list of tokenized sentences.\n",
    "    predictions (list): A list of predicted label sequences for each sentence.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary where keys are diseases (D) with descriptors and values are lists of treatments (T).\n",
    "    \"\"\"\n",
    "    disease_treatment_map = defaultdict(list)\n",
    "\n",
    "    def is_valid_entity(entity):\n",
    "        \"\"\"\n",
    "        Validate if the extracted entity is meaningful.\n",
    "\n",
    "        Parameters:\n",
    "        entity (str): The entity to validate.\n",
    "\n",
    "        Returns:\n",
    "        bool: True if the entity is valid, False otherwise.\n",
    "        \"\"\"\n",
    "        # Disallow entities with invalid characters or overly short entities\n",
    "        if re.search(r\"[()\\d]\", entity) or len(entity.split()) < 1 or re.match(r\"^[A-Z]\\.$\", entity):\n",
    "            return False\n",
    "        # Exclude overly generic terms\n",
    "        if entity.lower() in [\"disease\", \"cancer\", \"advanced disease\"]:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def is_valid_treatment(treatment):\n",
    "        \"\"\"\n",
    "        Validate if the extracted treatment is meaningful.\n",
    "\n",
    "        Parameters:\n",
    "        treatment (str): The treatment to validate.\n",
    "\n",
    "        Returns:\n",
    "        bool: True if the treatment is valid, False otherwise.\n",
    "        \"\"\"\n",
    "        # Exclude generic terms and overly short treatments\n",
    "        invalid_terms = {\"and\", \"with\", \"the\", \"of\"}\n",
    "        return treatment.isalpha() and len(treatment) > 2 and treatment.lower() not in invalid_terms\n",
    "\n",
    "    for sentence, prediction in zip(sentences, predictions):\n",
    "        # Convert the tokenized sentence into a spaCy Doc object for dependency parsing\n",
    "        doc = nlp(\" \".join(sentence))\n",
    "\n",
    "        current_disease = None\n",
    "        for idx, (word, label) in enumerate(zip(sentence, prediction)):\n",
    "            if label == \"D\":  # Identify disease\n",
    "                # Start forming a multi-word entity\n",
    "                token = doc[idx]\n",
    "                descriptor = set()\n",
    "\n",
    "                # Add adjectives or compound descriptors linked to the disease\n",
    "                for child in token.children:\n",
    "                    if child.dep_ in [\"amod\", \"compound\"] and child.pos_ in [\"ADJ\", \"NOUN\"]:\n",
    "                        descriptor.add(child.text)\n",
    "\n",
    "                # Check for preceding descriptors in the sentence\n",
    "                j = idx - 1\n",
    "                while j >= 0 and prediction[j] == \"O\":\n",
    "                    prev_token = doc[j]\n",
    "                    if prev_token.dep_ in [\"amod\", \"compound\"] and prev_token.pos_ in [\"ADJ\", \"NOUN\"]:\n",
    "                        descriptor.add(sentence[j])\n",
    "                    j -= 1\n",
    "\n",
    "                # Combine descriptor with the disease\n",
    "                descriptor_list = list(descriptor)\n",
    "                current_disease = \" \".join(descriptor_list + [word])\n",
    "\n",
    "                # Include subsequent words labeled as `D` to form a multi-word entity\n",
    "                k = idx + 1\n",
    "                while k < len(sentence) and prediction[k] == \"D\":\n",
    "                    current_disease += f\" {sentence[k]}\"\n",
    "                    k += 1\n",
    "\n",
    "                # Skip to the last word of the entity\n",
    "                idx = k - 1\n",
    "\n",
    "                # Validate disease entity\n",
    "                if not is_valid_entity(current_disease):\n",
    "                    current_disease = None\n",
    "\n",
    "            elif label == \"T\" and current_disease:  # Associate treatment with the disease\n",
    "                if is_valid_treatment(word):\n",
    "                    disease_treatment_map[current_disease].append(word)\n",
    "\n",
    "    # Post-process the map to remove non-alphabetic treatments and normalize phrases\n",
    "    final_map = {}\n",
    "    for disease, treatments in disease_treatment_map.items():\n",
    "        meaningful_treatments = list(set(t for t in treatments if is_valid_treatment(t)))  # Deduplicate treatments\n",
    "        if is_valid_entity(disease):\n",
    "            final_map[disease] = meaningful_treatments\n",
    "\n",
    "    return final_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract diseases and treatments using test sentences and predictions\n",
    "# disease_treatment_dict = extract_diseases_and_treatments(train_sentences, train_labels)\n",
    "disease_treatment_dict = extract_diseases_and_treatments(test_sentences, test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Complete Disease-Treatment Dictionary:\n",
      "|   # | Disease                                   | Treatments                                                                                          |\n",
      "|-----|-------------------------------------------|-----------------------------------------------------------------------------------------------------|\n",
      "|   1 | gestational diabetes cases                | glycemic, good, control                                                                             |\n",
      "|   2 | hereditary retinoblastoma                 | radiotherapy                                                                                        |\n",
      "|   3 | myocardial infarction                     | aspirin                                                                                             |\n",
      "|   4 | ulcer                                     | treatment, antibiotic, intravenous                                                                  |\n",
      "|   5 | hemorrhagic stroke                        | infusion, alteplase, accelerated                                                                    |\n",
      "|   6 | intracranial hemorrhage                   | method                                                                                              |\n",
      "|   7 | coronary left main artery                 | ventricular, anterior, wall                                                                         |\n",
      "|   8 | insemination partner preeclampsia         | insemination, donor                                                                                 |\n",
      "|   9 | severe hyperammonemia                     | chemotherapy, transplantation, organ                                                                |\n",
      "|  10 | pulmonary major embolism                  | hemodynamics                                                                                        |\n",
      "|  11 | mesothelioma                              | chemotherapy, thoracotomy, radiotherapy                                                             |\n",
      "|  12 | testicular bleeding                       | fine, aspiration, needle                                                                            |\n",
      "|  13 | extratunical haematomata                  | TESE                                                                                                |\n",
      "|  14 | azoospermia                               | TEFNA                                                                                               |\n",
      "|  15 | pulmonary primary hypertension            | dexfenfluramine, fenfluramine                                                                       |\n",
      "|  16 | nerve Cranial injuries                    | persistent, conduction, blocks                                                                      |\n",
      "|  17 | colorectal cancer                         | leucovorin, cisplatin                                                                               |\n",
      "|  18 | colds                                     | antibiotics                                                                                         |\n",
      "|  19 | nsclc                                     | treatment, surgical, got, chemoradiotherapy, radiotherapy                                           |\n",
      "|  20 | bos                                       | extracorporeal, therapy, photopheresis                                                              |\n",
      "|  21 | small la-nsclc                            | chemotherapy, radiotherapy                                                                          |\n",
      "|  22 | small patients                            | cisplatin, radiotherapy                                                                             |\n",
      "|  23 | lung carcinoma                            | videothoracoscopic, thoracotomy, lung, lobectomy, open, resection, partial                          |\n",
      "|  24 | metastasis                                | surgical, resection                                                                                 |\n",
      "|  25 | advanced brain clinical consecutive nsclc | chemotherapy, irinotecan, support, ifosfamide, cisplatin, combination                               |\n",
      "|  26 | metastatic colorectal cancer              | oxaliplatin, intravenous                                                                            |\n",
      "|  27 | lung primary cancer                       | resection                                                                                           |\n",
      "|  28 | symptomatic metastases                    | radiotherapy                                                                                        |\n",
      "|  29 | primary cancer                            | adjuvant, therapy, radiation                                                                        |\n",
      "|  30 | complete year among overall response sclc | carboplatin                                                                                         |\n",
      "|  31 | neck cancer                               | irradiation, therapy                                                                                |\n",
      "|  32 | Successful psoriasis                      | active, analogue, vitamin, alpha                                                                    |\n",
      "|  33 | melanoma                                  | leukocyte, Hoffmann, recombinant, Roche, interferon                                                 |\n",
      "|  34 | advanced stage                            | chemotherapy, methotrexate, every, combination, doxorubicin, bleomycin, program, weekly, consisting |\n",
      "|  35 | liver                                     | therapy, lanreotide                                                                                 |\n",
      "|  36 | symptomatic bronchiectasis                | antibronchoobstructive, chest, physical, therapy, medication, antibiotics                           |\n",
      "|  37 | biliary colic symptoms                    | cholecystectomy                                                                                     |\n",
      "|  38 | inflammatory infection                    | clarithromycin                                                                                      |\n",
      "|  39 | chronic bronchitis                        | amoxycillin                                                                                         |\n",
      "|  40 | infection                                 | clarithromycin, amoxicillin, omeprazole, combination                                                |\n",
      "|  41 | Contemporary asthma                       | corticosteroids                                                                                     |\n",
      "|  42 | viremia                                   | therapy, combination                                                                                |\n",
      "|  43 | CBD stones                                | surgical, exploration                                                                               |\n",
      "|  44 | severe hypoxemia                          | pulse, therapy, glucocorticoid                                                                      |\n",
      "|  45 | meningitis                                | vaccines                                                                                            |\n",
      "|  46 | breast cancer                             | undergone, mastectomy, subcutaneous                                                                 |\n",
      "|  47 | malignant melanoma                        | interferon                                                                                          |\n",
      "|  48 | esophageal achalasia                      | dilation, injection, myotomy, botulinum, laparoscopic, pneumatic, toxin                             |\n",
      "|  49 | irritable bowel syndrome                  | medicine, herbal, Chinese                                                                           |\n",
      "|  50 | cell sickle disease                       | hydroxyurea                                                                                         |\n",
      "|  51 | hemolytic autoimmune anemia               | heparin                                                                                             |\n",
      "|  52 | hepatitis B                               | vaccine                                                                                             |\n",
      "|  53 | deficiency                                | therapy                                                                                             |\n",
      "|  54 | hypertension                              | acid, ascorbic                                                                                      |\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nComplete Disease-Treatment Dictionary:\")\n",
    "table_data = [[i + 1, disease, ', '.join(treatments)] for i, (disease, treatments) in enumerate(disease_treatment_dict.items())]\n",
    "print(tabulate(table_data, headers=[\"#\", \"Disease\", \"Treatments\"], tablefmt=\"github\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# disease_treatment_data = pd.DataFrame(\n",
    "#     data = table_data,\n",
    "#     columns=[\"#\", \"Disease\", \"Treatments\"]\n",
    "# )\n",
    "\n",
    "# disease_treatment_data.set_index('#')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Summary of Disease-Treatment Dictionary**\n",
    "\n",
    "#### **Key Observations**\n",
    "\n",
    "1. **Accurate Disease-Treatment Pairs**:\n",
    "   - Diseases like **diabetes gestational cases**, **hereditary retinoblastoma**, and **myocardial infarction** are appropriately matched with treatments like `glycemic control`, `radiotherapy`, and `aspirin`.\n",
    "   - Common conditions such as **colds**, **viremia**, and **sickle cell disease** have accurate treatments like `antibiotics`, `combination therapy`, and `hydroxyurea`.\n",
    "\n",
    "2. **Ambiguous Treatments**:\n",
    "   - Some treatments lack specificity or refer to procedures or anatomical terms:\n",
    "     - **Intracranial hemorrhage** → `method` (not a specific treatment).\n",
    "     - **Inflammatory disorders** → `large, intestine` (describes anatomy, not a therapy).\n",
    "     - **CBD stones** → `exploration, surgical` (procedural, not specific).\n",
    "\n",
    "3. **Irrelevant or Noisy Entries**:\n",
    "   - Diseases like **deficiency** and **prevention** have vague or overly generic treatments (`therapy`, `vaccines, oral`), indicating potential noise in the data.\n",
    "\n",
    "4. **Multi-Word Diseases**:\n",
    "   - Verbose entries like **brain clinical consecutive advanced nsclc** and **response complete year among overall sclc** include excessive descriptors, complicating interpretation.\n",
    "   - Treatments such as `cisplatin` and `carboplatin` are relevant but could benefit from cleaner disease names.\n",
    "\n",
    "5. **Procedural and General Descriptions**:\n",
    "   - Terms like **primary cancer** and **advanced stage** lack specificity in both disease and treatment descriptions.\n",
    "   - Treatments like `resection` and `radiation` are procedural rather than therapeutic.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Key Findings**\n",
    "1. **Strengths**:\n",
    "   - Accurate matching of common diseases such as **diabetes**, **retinoblastoma**, and **colorectal cancer** with appropriate treatments.\n",
    "   - Identification of valid treatments for less common conditions like **sickle cell disease** and **meningitis**.\n",
    "\n",
    "2. **Weaknesses**:\n",
    "   - Noise in disease names (e.g., \"advanced stage\") and vague treatments (e.g., \"method\").\n",
    "   - Procedural terms (e.g., \"resection\", \"exploration\") dominate some entries, reducing specificity.\n",
    "   - General categories (e.g., \"deficiency\", \"prevention\") lack detailed context or relevance.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Conclusion**\n",
    "The dictionary captures accurate disease-treatment mappings for many common and rare conditions. However, improving clarity in disease names and filtering procedural or vague treatments is essential for enhancing usability. This refinement could focus on removing noise and ensuring specificity in disease-treatment relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation as per rubric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted treatments for the disease 'hereditary retinoblastoma': radiotherapy\n"
     ]
    }
   ],
   "source": [
    "# Display results for \"hereditary retinoblastoma\"\n",
    "specific_disease = \"hereditary retinoblastoma\"\n",
    "specific_treatments = disease_treatment_dict.get(specific_disease, [])\n",
    "\n",
    "if specific_treatments:\n",
    "    print(f\"Predicted treatments for the disease '{specific_disease}': {', '.join(specific_treatments)}\")\n",
    "else:\n",
    "    print(f\"No treatments found for the disease '{specific_disease}'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
